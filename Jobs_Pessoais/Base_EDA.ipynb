{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Data Collection and Exploratory Data Analysis (EDA)\n",
    "Data collection and EDA can be easily performed with Python due to the libraries used for data analytics, such as Numpy and Pandas. Hence, you will start by importing these libraries along with other visualization and statistical libraries that helped you during the later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas for reading data and performing other dataframe-related operations\n",
    "import pandas as pd\n",
    "\n",
    "# importing numpy for performing various numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# importing matplotlib, seaborn and plotly for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots\n",
    "import make_subplots\n",
    "\n",
    "# importing statistical libraries\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, you imported the data as a Pandas DataFrame and created a copy for data cleaning and downstream analytical steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing the sales data as a pandas DataFrame\n",
    "sales_data = pd.read_csv('sales_info.csv')\n",
    "\n",
    "# creating a copy of the data for analysis\n",
    "df = sales_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first obvious step was to examine the data, so you performed exploratory data analysis using Python. You dealt with sales data containing information about a company’s various products across multiple categories and countries. In addition to the product and transaction details, you also provided customer demographic information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# viewing the first few rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the data, you created a table describing each column and how it could be used during data analysis.\n",
    "\n",
    "Next, you calculated the data dimensions and found that you dealt with around 3.5K transactions and 19 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the number of rows and columns there in the data\n",
    "print(\"The number of rows are {} and the number of columns are {}\".format(df.shape[0],df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you had a basic understanding of the data, it was time to start with data cleaning. Data cleaning needs to focus on the following key aspects-\n",
    "\n",
    "Basic understanding of data\n",
    "Duplicates\n",
    "Incorrect Data Types\n",
    "Anomalies/Outliers\n",
    "Missing Values\n",
    "Structural Errors\n",
    "As you have already understood the data, start by checking for duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2) Data Cleaning\n",
    "Data cleaning is a crucial process in data analytics. It improves data quality, ensuring the insights gained from the analytical process are accurate, consistent, complete, reliable, and reproducible.\n",
    "\n",
    "Data can have errors, biases, and incomplete information, so numerous data-cleaning steps are performed before data analysis. This section will show you the various data-cleaning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Duplicate Removal\n",
    "\n",
    "It found that there were eight duplicate rows in the data.\n",
    "\n",
    "b. Type Casting\n",
    "\n",
    "Next, you check the data types in the columns. Several columns, such as `Date` and `Customer Estimated Income,` seemed to have the wrong data types.\n",
    "You will use drop.duplicated() command to remove them and ensure that the duplicates had gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A. checking if there are any duplicates\n",
    "df.duplicated().sum()\n",
    "\n",
    "# removing duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# re-checking if there are any duplicates\n",
    "df.duplicated().sum()\n",
    "\n",
    "\n",
    "#B. finding the data types and number of non-null values in the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why that is the case, you view and explore the unique values of all the columns. While for `Customer Estimated Income`, the presence of the $ symbol caused wrong data types, several other issues were present, such as invalid values masking missing values and inconsistent data (e.g., `Education`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find what all unwanted values are present in the different columns that are causing it to have incorrect datatypes\n",
    "\n",
    "# first creating a copy of the data that does not have missing values\n",
    "df_without_na = df.dropna().copy()\n",
    "\n",
    "# finding the unique values for each column in the data to find such unwanted values\n",
    "for i in df_without_na:\n",
    "print('\\n', i, df_without_na[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve the invalid values:\n",
    "1. note them down and replace them with missing ones so they can be dealt with through imputation.\n",
    "2. You remove the $ symbol from Income and change its data type to float.\n",
    "3. You also changed the data types of the remaining columns to their correct types, as the invalid values no longer caused any hindrance. Upon re-checking, all data types seemed to be correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. listing invalid values to be replaced with np.nan\n",
    "invalid_values = [\"%48224*#(\", \"(#23#(@!\", \"!!@#$%^&*\", \"??><<{}[]\", \"ABCDE\", \"NANANANA\", \"-\", \"???\"]\n",
    "\n",
    "# replacing invalid values with np.nan\n",
    "df = df.replace(invalid_values, np.nan)\n",
    "\n",
    "#2. removing the '$' sign from values in Estimated Income column and converting it to numeric.\n",
    "df['Customer Estimated Income'] = df['Customer Estimated Income'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "#3. typecasting columns to their correct data types\n",
    "df['Loyalty Balance'] = pd.to_numeric(df['Loyalty Balance'])\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# rechecking column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Anomaly/Outlier Treatment\n",
    "\n",
    "1. To detect outliers, you create a function that creates boxplots for specified columns.\n",
    "2. You will then create boxplots for the numeric columns. Upon inspection, you found that while a few columns had extreme/abnormally large values (as expected in sales data), some columns had anomalous values (e.g., `Customer Estimated Income` and `Customer Duration` with negative values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. creating function for boxplots\n",
    "def plot_boxplots(df, columns, num_cols=3):\n",
    "\n",
    "# calculating number of rows and columns for subplots\n",
    "num_rows = math.ceil(len(columns) / num_cols)\n",
    "\n",
    "# creating subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 6))\n",
    "\n",
    "# flattening the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# ploting boxplots for each specified column\n",
    "for i, col in enumerate(columns):\n",
    "sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "axes[i].set_title(f'Boxplot of {col}')\n",
    "\n",
    "# removing empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "fig.delaxes(axes[j])\n",
    "\n",
    "# showing plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#2. selecting numerical columns\n",
    "numerical_cols = df.drop('Year', axis=1).select_dtypes(include='number').columns\n",
    "\n",
    "# creating boxplots\n",
    "plot_boxplots(df, numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle such anomalies, you create a function that caps outliers using the Inter-Quartile (IQR) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to perform outlier capping\n",
    "def iqr_outlier_capping(df, columns):\n",
    "\n",
    "# creating a copy of the DataFrame to avoid modifying the original data\n",
    "df_capped = df.copy()\n",
    "for col in columns\n",
    "\n",
    "    # calculating Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df_capped[col].quantile(0.25)\n",
    "    Q3 = df_capped[col].quantile(0.75)\n",
    "\n",
    "    # calculating IQR (Interquartile Range)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # calculating lower and upper bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # capping the outliers\n",
    "    df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    return df_capped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will use the above-created function on columns with anomalies and recreate the boxplots for them to ensure that the anomalies were indeed gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing outlier capping with anomalous values\n",
    "columns_to_cap = ['Customer Duration', 'Loyalty Balance', 'Customer Estimated Income']\n",
    "df = iqr_outlier_capping(df, columns_to_cap)\n",
    "\n",
    "# re-creating boxplots for the columns where outlier capping is performed\n",
    "plot_boxplots(df, columns_to_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Missing Value Imputation\n",
    "\n",
    "One of the most crucial aspects of data cleaning is dealing with missing values. As you have already dealt with invalid values that were masking missing values, you now have a much better idea of the status of missing values in the data. Upon detection, you found several columns with missing values.\n",
    "You create a function that performs group-level missing value imputations using mean and mode for imputing numerical and categorical columns, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing column names with missing value\n",
    "print(df.isna().sum()[df.isna().sum()>0].index)\n",
    "\n",
    "# creating a function for imputing missing values in a target column using group-level mean/mode values\n",
    "def impute_missing_values(df, target_column, group_by_column, column_type):\n",
    "\n",
    "# making a copy of the dataframe to avoid modifying the original dataframe\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# ensuring target_column and group_by_column exist in the dataframe\n",
    "if target_column not in df_imputed.columns or group_by_column not in df_imputed.columns:\n",
    "    raise ValueError(\"Specified columns are not present in the dataframe.\")\n",
    "\n",
    "# imputing based on column type\n",
    "if column_type == 'numeric':\n",
    "\n",
    "\n",
    "    # checking if the target column is numeric\n",
    "    if not pd.api.types.is_numeric_dtype(df_imputed[target_column]):\n",
    "\n",
    "        raise TypeError(f\"Column '{target_column}' is not numeric. Please specify 'categorical' for non-numeric columns.\")\n",
    "    \n",
    "\n",
    "    # computing the mean value for each group and fill missing values\n",
    "    group_means = df_imputed.groupby(group_by_column)[target_column].transform('mean')\n",
    "    df_imputed[target_column] = df_imputed[target_column].fillna(group_means)\n",
    "elif column_type == 'categorical':\n",
    "\n",
    "    # checking if the target column is categorical\n",
    "    if pd.api.types.is_numeric_dtype(df_imputed[target_column]):\n",
    "        raise TypeError(f\"Column '{target_column}' is numeric. Please specify 'numeric' for numeric columns.\")\n",
    "\n",
    "    # computing the mode value for each group and fill missing values\n",
    "    mode_values = df_imputed.groupby(group_by_column)[target_column].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n",
    "    df_imputed[target_column] = df_imputed[target_column].fillna(mode_values)\n",
    "else:\n",
    "    raise ValueError(\"Invalid column type specified. Use 'numeric' or 'categorical'.\")\n",
    "return df_imputed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You then used the function to impute missing values and used appropriate group columns. Upon re-checking, it became evident that the data was free of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the function above to perform missing value imputation for numerical columns\n",
    "df = impute_missing_values(df, target_column = 'Customer Estimated Income', group_by_column = 'State', column_type = 'numeric')\n",
    "df = impute_missing_values(df, target_column = 'Loyalty Balance', group_by_column = 'Country', column_type = 'numeric')\n",
    "df = impute_missing_values(df, target_column = 'Customer Duration', group_by_column = 'Customer Gender', column_type = 'numeric')\n",
    "\n",
    "# using the function above to perform missing value imputation for categorical columns\n",
    "df = impute_missing_values(df, target_column = 'Customer Education Level', group_by_column = 'Customer Gender', column_type = 'categorical')\n",
    "df = impute_missing_values(df, target_column = 'Customer Marital Status', group_by_column = 'Country', column_type = 'categorical')\n",
    "\n",
    "# rechecking if there are any missing values left\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Fixing Inconsistent Data\n",
    "\n",
    "During typecasting, you found some structural issues in the data with the `Customer Education Level` column having inconsistent categories. Upon carefully looking at the categories and their counts, you found that some categories are mentioned differently (e.g., PhD being also mentioned as Research Degree and Doctorate).\n",
    "To resolve this, you removed the inconsistent categories by standardizing them, eventually providing you with only four education levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking the different categories in the Education Level column\n",
    "df['Customer Education Level'].value_counts().sort_index()\n",
    "\n",
    "# mapping for standardizing categorical values\n",
    "education_mapping = {\n",
    "'Bachelor':'Bachelor',\n",
    "'Doctorate':'PhD',\n",
    "'Graduate Degree':'Master',\n",
    "'High School':'High School',\n",
    "'Master':'Master',\n",
    "'Masters Diploma':'Master',\n",
    "'PhD':'PhD',\n",
    "'Research Degree':'PhD',\n",
    "'Senior Secondary Education':'High School',\n",
    "'Undergrad':'Bachelor',\n",
    "'Undergraduate Degree':'Bachelor'\n",
    "}\n",
    "\n",
    "# standardizing the 'Customer Education Level' column\n",
    "df['Customer Education Level'] = df['Customer Education Level'].map(education_mapping)\n",
    "\n",
    "# rechecking if the categories have been fixed\n",
    "df['Customer Education Level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is now clean, it’s time to analyze it. To perform data analysis using Python, you must have your data as a Pandas DataFrame, as it is much easier to deal with. As you already had the data in this form, you had multiple ways to analyze data.\n",
    "\n",
    "In Python, you have data manipulation libraries like Pandas and Numpy, visualization libraries like Matplotlib, Seaborn, and Plotly, and statistical libraries like Scipy and Statsmodels. All such libraries allow you to look at the data from various perspectives and uncover hidden patterns.\n",
    "\n",
    "The following sections will show all the libraries, functions, and techniques for slicing and diceing the data. Note that the focus will not be on the insights gained from the analysis, as discussed in the ‘How to communicate your insights’ section, which involves reporting. Therefore, the aim will be to understand how Python can be leveraged and the approach to be taken when performing data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Data Mining\n",
    "a. The simplest and easiest way to analyze data is to mine it. This involves extracting key valuable information about the columns. Data analytics with Python is easy because of libraries like Pandas. If you have your data as a Pandas DataFrame, you can easily use the describe function, which provides statistical information about the columns.\n",
    "b. The same function can also be applied to finding key insights about the categorical columns.\n",
    "c. It was necessary to derive a few important variables to mine the data further. These included calculating the year-month combination of the transaction, profit, profit margin, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. finding the key statistical values of the numerical columns\n",
    "df.describe().T\n",
    "\n",
    "#B. # finding the key statistical values of the categorical columns\n",
    "df.drop('Date', axis=1).describe(exclude=np.number).T \n",
    "\n",
    "#C. Creating a year-month variable for easily understanding the date related columns\n",
    "# dropping the year and month column as this information is already there in the Date column\n",
    "df = df.drop(['Year','Month'],axis=1)\n",
    "\n",
    "# extracting year and creating a Year column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# extracting year and month and creating a Year-Month column\n",
    "df['Year_Month'] = df['Date'].dt.strftime('%Y-%m')\n",
    "\n",
    "# calculating profit\n",
    "df['Profit'] = df['Revenue'] - df['Cost']\n",
    "\n",
    "# calculating unit profit\n",
    "df['Unit Profit']=df['Unit Price']-df['Unit Cost']\n",
    "\n",
    "# calculating profit margin\n",
    "df['Profit Margins'] = df['Profit'] / df['Revenue']\n",
    "\n",
    "# calculating unit profit margin\n",
    "df['Unit Profit Margin']= df['Unit Profit']/df['Unit Price']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once such key variables become available, key performance metrics (KPI) can be easily calculated using Python, as performing arithmetic operations between different columns is extremely easy with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saes KPIs\n",
    "ttl_revenue = df['Revenue'].sum()\n",
    "ttl_profit = df['Profit'].sum()\n",
    "ttl_revenue_previous = df[df['Year'] == 2015]['Revenue'].sum()\n",
    "ttl_revenue_current = df[df['Year'] == 2016]['Revenue'].sum()\n",
    "rga = (ttl_revenue_current-ttl_revenue_previous)/ttl_revenue_previous\n",
    "pm = (ttl_profit/ttl_revenue)*100\n",
    "aov = ttl_revenue/len(df)\n",
    "\n",
    "# Customer KPIs\n",
    "avg_age = df['Customer Age'].mean()\n",
    "median_income = df['Customer Estimated Income'].median()\n",
    "mode_martl_status = df['Customer Marital Status'].mode()[0]\n",
    "mode_edu_level = df['Customer Education Level'].mode()[0]\n",
    "male_percentage = (df[df['Customer Gender'] == 'M'].shape[0] / df.shape[0]) * 100\n",
    "female_percentage = (df[df['Customer Gender'] == 'F'].shape[0] / df.shape[0]) * 100\n",
    "\n",
    "# Product KPIs\n",
    "top_selling_prod = df.groupby('Sub Category')['Revenue'].sum().sort_values(ascending=False).index[0]\n",
    "most_purchased_prod = df.groupby('Sub Category')['Quantity'].sum().sort_values(ascending=False).index[0]\n",
    "most_costly_prod = df.groupby('Sub Category')['Cost'].sum().sort_values(ascending=False).index[0]\n",
    "most_profitable_prod = df.groupby('Sub Category')['Profit'].sum().sort_values(ascending=False).index[0]\n",
    "most_margin_prod = df.groupby('Sub Category')['Profit Margins'].sum().sort_values(ascending=False).index[0]\n",
    "\n",
    "# Operational KPIs\n",
    "ttl_cost = df['Cost'].sum()\n",
    "avg_basket_size = df['Quantity'].mean()\n",
    "avg_profit_margin = df['Profit Margins'].mean()\n",
    "top_costly_country = df.groupby('Country')['Cost'].sum().sort_values(ascending=False).index[0]\n",
    "\n",
    "# Print statements for KPIs\n",
    "print(f\"Total Revenue: ${ttl_revenue:,.2f}\")\n",
    "print(f\"Total Profit: ${ttl_profit:,.2f}\")\n",
    "print(f\"Revenue Growth Rate (2015 to 2016): {rga:.2%}\")\n",
    "print(f\"Profit Margin: {pm:.2f}%\")\n",
    "print(f\"Average Order Value: ${aov:,.2f}\")\n",
    "\n",
    "print(\"\\nCustomer KPIs:\")\n",
    "print(f\"Average Customer Age: {avg_age:.1f} years\")\n",
    "print(f\"Median Customer Estimated Income: ${median_income:,.2f}\")\n",
    "print(f\"Most Common Marital Status: {mode_martl_status}\")\n",
    "print(f\"Most Common Education Level: {mode_edu_level}\")\n",
    "print(f\"Percentage of Male Customers: {male_percentage:.2f}%\")\n",
    "print(f\"Percentage of Female Customers: {female_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\nProduct KPIs:\")\n",
    "print(f\"Top Selling Product Sub-Category: {top_selling_prod}\")\n",
    "print(f\"Most Purchased Product Sub-Category: {most_purchased_prod}\")\n",
    "print(f\"Most Costly Product Sub-Category: {most_costly_prod}\")\n",
    "print(f\"Most Profitable Product Sub-Category: {most_profitable_prod}\")\n",
    "print(f\"Product Sub-Category with Highest Profit Margin: {most_margin_prod}\")\n",
    "\n",
    "print(\"\\nOperational KPIs:\")\n",
    "print(f\"Total Cost: ${ttl_cost:,.2f}\")\n",
    "print(f\"Average Basket Size: {avg_basket_size:.2f} items\")\n",
    "print(f\"Average Profit Margin: {avg_profit_margin:.2f}%\")\n",
    "print(f\"Country with Highest Total Cost: {top_costly_country}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll now do some complex analysis and calculate crucial financial information for different products. You will do this by grouping the dataframe by product categories and sub-categories and aggregating it by calculating the sum of the key financial columns like revenue, cost, profit, and margin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data by product category and sub-category and aggregating by revenue and cost by summing them\n",
    "prod_analysis_df = df.groupby(['Product Category','Sub Category'])[['Revenue','Cost']].sum().reset_index()\n",
    "\n",
    "# calculating profit and profit margin\n",
    "prod_analysis_df['Profits']= prod_analysis_df['Revenue'] - prod_analysis_df['Cost']\n",
    "prod_analysis_df['Profit Margins']= prod_analysis_df['Profits']/ prod_analysis_df['Revenue']\n",
    "\n",
    "# viewing data\n",
    "prod_analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can also calculate this information and more for different countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data by country with specific aggregation functions for each column\n",
    "aggregation_functions = {\n",
    "'Revenue': 'sum',\n",
    "'Cost': 'sum',\n",
    "'Quantity': 'sum',\n",
    "'Profit': 'sum',\n",
    "'Profit Margins': 'mean'\n",
    "}\n",
    "country_analysis_df = df.groupby('Country').agg(aggregation_functions).reset_index()\n",
    "\n",
    "# calculating the total sums for the columns to get proportions\n",
    "total_sums = country_analysis_df[['Revenue', 'Cost', 'Quantity', 'Profit', 'Profit Margins']].sum()\n",
    "\n",
    "# calculating proportions and adding them as new columns\n",
    "country_analysis_df['Revenue Proportion'] = round(country_analysis_df['Revenue'] / total_sums['Revenue'],3)*100\n",
    "country_analysis_df['Cost Proportion'] = round(country_analysis_df['Cost'] / total_sums['Cost'],3)*100\n",
    "country_analysis_df['Quantity Proportion'] = round(country_analysis_df['Quantity'] / total_sums['Quantity'],3)*100\n",
    "country_analysis_df['Profit Proportion'] = round(country_analysis_df['Profit'] / total_sums['Profit'],3)*100\n",
    "country_analysis_df['Profit Margins Proportion'] = round(country_analysis_df['Profit Margins'] / total_sums['Profit Margins'],3)*100\n",
    "\n",
    "# rearranging columns to place similar columns next to each other\n",
    "country_analysis_df = country_analysis_df[['Country',\n",
    "'Revenue', 'Revenue Proportion',\n",
    "'Cost', 'Cost Proportion',\n",
    "'Quantity', 'Quantity Proportion',\n",
    "'Profit', 'Profit Proportion',\n",
    "'Profit Margins', 'Profit Margins Proportion']]\n",
    "\n",
    "# viewing the data\n",
    "country_analysis_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
